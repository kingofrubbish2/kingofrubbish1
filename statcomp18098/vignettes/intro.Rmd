---
title: "Homework-2018.09.21"
author: "by 18089"
date: "2018/09/21"
output: html_document
---

# exercises 3.5

A discrete random variable X has probability mass function 
             x   0   1   2   3   4
           ------------------------
           p(x) 0.1 0.2 0.2 0.2 0.3

Use the inverse transform method to generate a random sample of size  1000 from the distribution of X.Construct a relative frequency table and compare the empirical with the theoretical probabilities.Repeat using the R sample function.

# answer

```{r}
inv_F<-function(x){
  if(x<=0.1)
    F<-0
  else if(x<=0.3)
    F<-1
  else if(x<=0.5)
    F<-2
  else if(x<=0.7)
    F<-3
  else if(x<=1)
    F<-4
  return(F)
} #Use the inverse transform method
x<-runif(1000) #generate a uniform distribution sample
sam<-sapply(x, inv_F)
p0<-length(which(sam==0))/1000
p1<-length(which(sam==1))/1000
p2<-length(which(sam==2))/1000
p3<-length(which(sam==3))/1000
p4<-length(which(sam==4))/1000 #get the empirical probabilities
## relative frequency table
table(sam)
## the empirical probabilities
p0
p1
p2
p3
p4
## the empirical probabilities are close to the corresponding theoretical probabilities
## Repeat using the R sample function
x<-runif(1000) #generate a uniform distribution sample
sam<-sapply(x, inv_F)
p0<-length(which(sam==0))/1000
p1<-length(which(sam==1))/1000
p2<-length(which(sam==2))/1000
p3<-length(which(sam==3))/1000
p4<-length(which(sam==4))/1000 #get the empirical probabilities
## relative frequency table
table(sam)
## the empirical probabilities
p0
p1
p2
p3
p4
## the empirical probabilities are close to the corresponding theoretical probabilities
```

# exercise 3.7

Write a function to generate a random sample of size n from the Beta(a,b) distribution by the acceptance-rejection method.Generate a random sample of size 1000 from the Beta(3,2) distribution.Graph the histogram of the sample with the theoretical Beta(3,2) density superimposed.

# answer

```{r}
fun1<-function(n,a,b){
j<-k<-0;
y <- numeric(n) 
while (k < n)
  { u <- runif(1) 
  j <- j + 1 
  x <- runif(1) #random variate from g 
  if (x^(a-1)* (1-x)^(b-1) > u)
    { 
    #we accept x 
    k <- k + 1 
    y[k] <- x 
    }
 } 
j
}#a function to generate a random sample
fun1(1000,3,2)
#the Beta(3,2)
n<-1000
j<-k<-0
y <- numeric(n) 
while (k < n)
  { u <- runif(1) 
  j <- j + 1 
  x <- runif(1) #random variate from g 
  if (x*x* (1-x) > u)
    { 
    #we accept x 
    k <- k + 1 
    y[k] <- x 
    }
 } 
j
hist(y,prob = TRUE)#histogram of the sample
z <- seq(0, 1, 0.01) 
lines(z, 12*z*z*(1-z))#the theoretical Beta(3,2) density superimposed
```

# exercise 3.12

Simulate a continuous Exponential-Gamma mixture.Suppose that the rate parameter ?? has Gamma(r,??) distribution and Y has Exp(??) distribution.That is,(Y|??=??)~fY(y|??)=??e^(-??y).Generate 1000 random observations from this mixture with r=4 and ??=2.

# answer
```{r}
n <- 1000
r <- 4
beta <- 2
lambda <- rgamma(n, r, beta) #Gamma(r,??) distribution
x <- rexp(n, lambda)#Exp(??) distribution
x
```


---
title: "Homework-2018.09.28"
author: "by 18089"
date: "2018.10.09"
output: html_document
---

# question

### exercise 5.4

Write a function to compute a Monto Carlo estimate of the Beta(3,3) cdf,and use the function to estimate F(x) for x=0.1,0.2,...,0.9.Compare the estimates with the values returned by the pbeta function in R. 


# answer

First,we can write the Beta(3,3) cdf.

$$\int_{0}^{x}30t^2(1-t)^2 dt$$
So,we get the Monto Carlo estimate is :
```{r}
MCF<-function(a){
  m<-1e4
  t<-runif(m,0,a)# get the uniform sample
  theta.hat <- mean(30*a*t^2*(1-t)^2)# get the Monto Carlo estimate
  print(theta.hat)
}
pv<-c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9)
for(x in pv){
 print(x)
 MCF(x)
 print(pbeta(x,3,3))#use the pbeta function and compare with MCF
}
```
# question

### exercise 5.9

The Rayleigh density [156, (18.76)] is 
$$f(x)=\frac{x}{\sigma^2}e^\frac{-x^2}{2\sigma^2}\quad x>=0, \sigma>0.$$

Implement a function to generate samples from a Rayleigh(??) distribution.using antithetic variables. What is the percent reduction in variance of $\frac{X+X'}{2}$ compared with $\frac{X1+X2}{2}$ for independent X1, X2?

# answer

We can get Rayleigh(??) distribution by the Rayleigh density.That is

$$F(x)=1-e^\frac{-x^2}{2\sigma^2}\quad x>=0, \sigma>0.$$
So,we can get the inverse function is 

$$G(x)=\sqrt{-2\sigma^2ln(1-x)}\quad X=U(0,1).$$
by using antithetic variables,we can get samples from the Rayleigh(??) distribution.

$$\frac{X+X'}{2}=\frac{\sqrt{-2\sigma^2ln(1-x)}+\sqrt{-2\sigma^2ln(x)}}{2}\quad X=U(0,1).$$
```{r}
m<-1e4
x<-runif(m,0,1)
a<-(sqrt(-2*log(1-x))+sqrt(-2*log(x)))/2
b<-sqrt(-2*log(1-x))
c(var(a),var(b),var(b)/var(a))#compare the results
```

# question

### exercise 5.13

Find two importance functions f1 and f2 that are supported on (1,??) and are ??close?? to

$$g(x)=\frac{x^2}{\sqrt{2\pi}}e^\frac{-x^2}{2}\quad x>1.$$
Which of your two importance functions should produce the smaller variance in estimating 

$$\int_{1}^{??}\frac{x^2}{\sqrt{2\pi}}e^\frac{-x^2}{2}dx$$
by importance sampling? Explain.


# answer

We can get two importance functions f1 and f2

$$f1(x)=\frac{1}{x^2}\quad x>1.$$

$$f2(x)=xe^\frac{1-x^2}{2}\quad x>1.$$
So,we can get the cdf

$$F1(x)=1-\frac{1}{x}\quad x>1.$$

$$F2(x)=1-e^\frac{1-x^2}{2}\quad x>1.$$
So,we can get the inverse function

$$H1(x)=\frac{1}{1-x}\quad X=U(0,1).$$

$$H2(x)=\sqrt{1-2ln(1-x)}\quad X=U(0,1).$$
```{r}
m<-1e4
x<-runif(m,0,1)
a<-1/(1-x)
b<-sqrt(1-2*log(1-x))
theta1<-sum(a^4*exp(-a^2/2))/sqrt(2*pi)/m#get the mean
theta2<-sum(b/sqrt(2*pi*exp(1)))/m
print(c(theta1,theta2))
var1<-var(a^4*exp(-a^2/2)/sqrt(2*pi))#get the variance
var2<-var(b/sqrt(2*pi*exp(1)))
print(c(var1,var2))
```
So we can find the same mean,but f2 has the smaller variance.
That is,f2 is the better importance function.


# question

### exercise 5.14

Obtain a Monte Carlo estimate of
$$\int_{1}^{??}\frac{x^2}{\sqrt{2\pi}}e^\frac{-x^2}{2}dx$$
by importance sampling.

# answer

We can get the importance functions f

$$f(x)=xe^\frac{1-x^2}{2}\quad x>1.$$
So,we can get the cdf

$$F(x)=1-e^\frac{1-x^2}{2}\quad x>1.$$
So,we can get the inverse function

$$H(x)=\sqrt{1-2ln(1-x)}\quad X=U(0,1).$$
So,
$$g(x)/f(x)=\frac{x}{\sqrt{2e\pi}}\quad x>1.$$

```{r}
m<-1e4
x<-runif(m,0,1)
b<-sqrt(1-2*log(1-x))
theta<-sum(b/sqrt(2*pi*exp(1)))/m#get the mean
print(theta)
var2<-var(b/sqrt(2*pi*exp(1)))#get the variance
print(var2)
```


---
title: "Homework-2018.11.02"
author: "by 18089"
date: "2018.11.06"
output: html_document
---

# Question

### Exercise 7.1

Compute a jackknife estimate of the bias and the standard error of the correlation statistic in Example 7.2. 


# Answer

```{r}
library(bootstrap)
n<-nrow(law)    #sample size
theta.jack<-numeric(n) #storage for replicates
b.cor<-function(x,i) cor(x[i,1],x[i,2])
theta.hat<-b.cor(law,1:n)
for(i in 1:n){
  theta.jack[i]<-b.cor(law,(1:n)[-i])
}
bias.jack<-(n-1)*(mean(theta.jack)-theta.hat) 
se.jack<-sqrt((n-1)*mean((theta.jack-theta.hat)^2)) 
round(c(original=theta.hat,bias=bias.jack, se=se.jack),3)
```



### Exercise 7.4

Refer to Exercise 7.4. Compute 95% bootstrap con???dence intervals for the mean time between failures 1/?? by the standard normal, basic, percentile, and BCa methods. Compare the intervals and explain why they may di???er.


# Answer

```{r}
library(boot)
set.seed(1)
n<-nrow(aircondit) #sample size
mean(aircondit[,1])
boot.mean<-function(x,i) mean(x[i,1]) #give the mean function
boot.out<-boot(data=aircondit,statistic=boot.mean,R=1000)
boot.ci(boot.out,conf=0.95)
```
we can find 95% bootstrap con???dence intervals are different,because we use different test types.


### Exercise 7.8


Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard error of $\hat{\theta}$.



# Answer


```{r}
library(bootstrap)
n<-nrow(scor)
theta.jack<-numeric(n)
n<-nrow(scor)
theta.jack<-numeric(n)
scor.pr<-function(x,i){
  a<-eigen(cor(x[i,]))
  a$values[1]/sum(a$values) # the proportion of variance 
}
theta.hat<-scor.pr(scor,1:n)
for(j in 1:n){
  theta.jack[j]<-scor.pr(scor,(1:n)[-j])
}
bias.jack<-(n-1)*(mean(theta.jack)-theta.hat) 
se.jack<-sqrt((n-1)*mean((theta.jack-theta.hat)^2)) 
round(c(original=theta.hat,bias=bias.jack, se=se.jack),3)
```



### Exercise 7.11

In Example 7.18, leave-one-out (n-fold) cross validation was used to select the best ???tting model. Use leave-two-out cross validation to compare the models.


# Answer

```{r}
library(DAAG)
attach(ironslag)
n <- length(magnetic) #in DAAG ironslag 
e1 <- e2 <- e3 <- e4 <- numeric((n-1)*n) #the size of samples
k=1 
for(i in 1:(n-1)){
  for(j in (i+1):n){   
  y<- magnetic[-i][-(j-1)]
  x<- chemical[-i][-(j-1)] #leave-two-out samples

  
  J1 <- lm(y ~ x) 
  yhat1 <- J1$coef[1] + J1$coef[2] * chemical[i] 
  e1[k] <- magnetic[i] - yhat1  #check out the modle
  
  J2 <- lm(y ~ x + I(x^2)) 
  yhat2 <- J2$coef[1] + J2$coef[2] * chemical[i] + J2$coef[3] * chemical[i]^2 
  e2[k] <- magnetic[i] - yhat2
  
  J3 <- lm(log(y) ~ x) 
  logyhat3 <- J3$coef[1] + J3$coef[2] * chemical[i] 
  yhat3 <- exp(logyhat3) 
  e3[k] <- magnetic[i] - yhat3
  
  J4 <- lm(log(y) ~ log(x)) 
  logyhat4 <- J4$coef[1] + J4$coef[2] * log(chemical[i]) 
  yhat4 <- exp(logyhat4) 
  e4[k] <- magnetic[i] - yhat4
  
  k<-k+1

  yhat1 <- J1$coef[1] + J1$coef[2] * chemical[j] 
  e1[k] <- magnetic[j] - yhat1 
  
  yhat2 <- J2$coef[1] + J2$coef[2] * chemical[j] + J2$coef[3] * chemical[j]^2 
  e2[k] <- magnetic[j] - yhat2
  
  logyhat3 <- J3$coef[1] + J3$coef[2] * chemical[j] 
  yhat3 <- exp(logyhat3) 
  e3[k] <- magnetic[j] - yhat3
  
  logyhat4 <- J4$coef[1] + J4$coef[2] * log(chemical[j]) 
  yhat4 <- exp(logyhat4) 
  e4[k] <- magnetic[j] - yhat4
  
  k<-k+1
  
  }
}
c(mean(e1^2), mean(e2^2), mean(e3^2), mean(e4^2)) #get the square error
lm(formula = magnetic ~ chemical + I(chemical^2))
```


According to the prediction error criterion, Model 2, the quadratic model, would be the best ???t for the data.
The ???tted regression equation for Model 2 is
$$\hat{Y}=24.49262-1.39334X+0.05452X^2$$

Compare with the leave-one-out (n-fold) cross validation,the leave-two-out cross validation has same conclusion,but more complicated,more accurate.


---
title: "Homework-2018.11.02"
author: "by 18089"
date: "2018.11.29"
output: html_document
---


# Question

### Exercise 9.6

For exercise 9.6, use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until the chain has converged approximately to the target distribution according to $\hat{R}<1.2$.

# Answer
```{r}
b=1000
m=10000
N=b+m
Gelman.Rubin <- function(psi){ 
  # psi[i,j] is the statistic psi(X[i,1:j]) 
  # for chain in i-th row of X 
  psi <- as.matrix(psi) 
  n <- ncol(psi) 
  k <- nrow(psi)
  psi.means <- rowMeans(psi) #row means 
  B <- n * var(psi.means) #between variance est. 
  psi.w <- apply(psi, 1, "var") #within variances 
  W <- mean(psi.w) #within est. 
  v.hat <- W*(n-1)/n + (B/(n*k)) #upper variance est. 
  r.hat <- v.hat / W #G-R statistic return(r.hat) 
}
iden<-function(theta){
  125*log(2+theta)+38*log(1-theta)+34*log(theta)
}
MCMC<-function(x,print_acc=F){
  y=1:N
  acc=0
  for(i in 1:N){
    p=runif(1)
    y[i]=x=if(runif(1)<exp(iden(p)-iden(x))){acc=acc+1;p}else x
  }
  if(print_acc) print(acc/N)
  y[(b+1):N]
}
plot(MCMC(0.5,print_acc = T))
M1=cumsum((MC1=MCMC(0.2)))/1:m
M2=cumsum((MC2=MCMC(0.4)))/1:m
M3=cumsum((MC3=MCMC(0.6)))/1:m
M4=cumsum((MC4=MCMC(0.8)))/1:m
psi=rbind(M1,M2,M3,M4)
plot((R=sapply(1:100,function(i)Gelman.Rubin(psi[,1:i]))),main="R value of Gelman.Rubin method",ylim=c(1,2))

c(mean(c(MC1[1:10000],MC2[1:10000],MC3[1:10000],MC4[1:10000])),var(c(MC1[1:10000],MC2[1:10000],MC3[1:10000],MC4[1:10000])))
```

### exercise 11.4

Find the intersection points A(k) in 

$$(0,\sqrt{k})$$

f the curves

$$S_{k-1}(a)=P\big(t(k-1)>\sqrt{\frac{a^2(k-1)}{k-a^2}}\big)$$
and

$$S_{k}(a)=P\big(t(k)>\sqrt{\frac{a^2k}{k+1-a^2}}\big)$$
for k =4:25,100,500,1000, where t(k) is a Student t random variable with k degrees of freedom. (These intersection points determine the critical values for a t-test for scale-mixture errors proposed by Sz??ekely [260].)

```{r}
alpha<-rep(1,25)
b<-c(4:25,100,500,1000)
for(i in 1:25){
  k=b[i]
inter<-function(a){
  pt(sqrt(a*a*(k-1)/(k-a*a)),df=k-1,lower.tail=F,log.p=T)-pt(sqrt(a*a*k/(k+1-a*a)),df=k,lower.tail=F,log.p=T)
}
solution <- uniroot(inter,lower=0.0001,upper=sqrt(k)-0.0001)
alpha[i]<- solution$root
print(alpha[i])
}#intersection points
```

---
title: "A-18098-2018-11-30"
author: "By 18098"
date: "2018/12/7"
output: html_document
---

Question

11.6

Write a function to compute the cdf of the Cauchy distribution, which has density

$$ 1/\theta\pi(1+[x-\eta/\theta]^2)$$

$$-\infty<x<\infty$$

where θ > 0. Compare your results to the   results from the R function pcauchy. (Also see the source code in pcauchy.c.)

2.A-B-O blood type problem

Let the three alleles be A, B, and O

Genotype|AA| BB | OO | AO  | BO | AB | SUM |
------|------|------|------|------|------|------|
Frequency|P2 | Q2 | r2 | 2pr | 2qr| 2pq| 1   |
Count    |nAA | nBB|nOO | nAO | nBO| nAB| n   |

Observed data: nA· = nAA + nAO = 28 (A-type),
nB· = nBB + nBO = 24 (B-type), nOO = 41 (O-type), nAB = 70 (AB-type).

Use EM algorithm to solve MLE of p and q (consider missing data nAA and nBB).

Record the maximum likelihood values in M-steps, are they increasing?

```{r}
##11.6
theta<-1
eta<-2
pcd<-function(x,theta,eta){
  integrate(function(y){1/(3.14*theta*(1+((y-eta)/theta)^2))},-Inf,x)}##the cdf of cauchy density

p1<-pcd(4,1,2)
p1
p<-pcauchy(4,1,2)
p## compare the consequence between the pcauchy and cdf of cauchy 

```

```{r}
##A-B-O blood type problem

na.<-28
nb.<-24
noo<-41
nab<-70
# 随机初始两个未知量
set.seed(123)
p<-runif(1,0,1)
q<-runif(1,0,1)
naa<-round(runif(1,1,28))
nbb<-round(runif(1,1,24))
i<-1
nonstop<-TRUE
while (nonstop) { 
  # E步骤，根据假设的p,q来算naa,nbb
naa<-c(naa,(28*p[i]^2)/(p[i]^2+2*p[i]*q[i]))
nbb<-c(nbb,(24/(q[i]^2+q[i]))*q[i]^2)
 # M步骤，根据上面算出的naa.nbb再来计算p,q
p<-c(p,sqrt(naa[i])) 
q<-c(q,nbb[i]/(24-nbb[i]))
i= i + 1
nonstop <-((p[i]-p[i-1])<10^(-10))}

print(cbind(p,q))    
```



---
title: "A-18098-2018-12-07"
author: "By 18098"
date: "2018/12/14"
output: html_document
---

Question:

3. 
    Use both for loops and lapply() to fit linear models to the
mtcars using the formulas stored in this list:
formulas <- list(mpg ~ disp,mpg ~ I(1 / disp), mpg~disp+wt, mpg~I(1/disp)+wt)

4.
   Fit the model mpg ~ disp to each of the bootstrap replicates of mtcars in the list below by using a for loop and lapply(). Can you do it without an anonymous function?
       bootstraps <- lapply(1:10, function(i) {
       rows <- sample(1:nrow(mtcars), rep = TRUE)
         mtcars[rows, ]})

5. 
    For each model in the previous two exercises, extract R2 using the function below.
rsq <- function(mod) summary(mod)$r.squared

3. 
    The following code simulates the performance of a t-test for non-normal data. Use sapply() and an anonymous function to extract the p-value from every trial.
       trials <- replicate(
         100,
         t.test(rpois(10, 10), rpois(7, 10)),
         simplify = FALSE
       )
Extra challenge: get rid of the anonymous function by using [[ directly.


6. 
    Implement a combination of Map() and vapply() to create an lapply() variant that iterates in parallel over all of its inputs and stores its outputs in a vector (or a matrix). What argu- ments should the function take?

Answer

```{r}
attach(mtcars)
a<-data.frame(disp,I(1 / disp),disp+wt,I(1/disp)+wt)
lmf<-function(x){
  return(lm(mpg~x))}
unlist(lapply(seq_along(a), function(i) {lapply(a[i],lmf)}))
```

```{r}
##4.
 bootstraps <- lapply(1:10, function(i) {
         rows <- sample(1:nrow(mtcars), rep = TRUE)
         mtcars[rows, ]
})
for (i in 1:10){
print(lm(bootstraps[[i]]$mpg~bootstraps[[i]]$disp))
}##loop
print(lapply(bootstraps, function(x) lm(mpg ~ disp,x)))##lapply
```


```{r}
##5.
formulas <- list(
mpg ~ disp,
mpg ~ I(1 / disp),
mpg ~ disp + wt,
mpg ~ I(1 / disp) + wt
)
o1 <- vector("list", length(formulas))
o2 <- vector("list", length(bootstraps))
(o1<-lapply(formulas,function(x) lm(x,mtcars)))
(o2 <- lapply(bootstraps, function(x) lm(mpg ~ disp,x)))

o <- c(o1,o2) 
rsq <- function(mod) summary(mod)$r.squared
(R_sqr <- sapply(o,rsq))

```

```{r}
##3.
 trials <- replicate(
         100,
         t.test(rpois(10, 10), rpois(7, 10)),
         simplify = FALSE
       )
sapply(trials, function(x) x$p.value)
sapply(trials, `[[`, 'p.value')
```

```{r}
##6.
library(parallel)
mcvMap <- function(f, FUN.VALUE , ...) {
    out <- mcMap(f, ...)
    vapply(out, identity, FUN.VALUE)
}
```























